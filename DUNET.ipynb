{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from audiomentations import (\n",
    "    Compose, AddBackgroundNoise\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import tarfile\n",
    "import io\n",
    "import soundfile as sf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import convolve\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "from audiomentations import Compose, AddBackgroundNoise\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(threshold=torch.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project = \"etri\",\n",
    "\n",
    "    config = {\n",
    "\n",
    "        \"DENOISER_LR\": 3e-4,\n",
    "        \"CLASSIFIER_LR\": 3e-4,\n",
    "\n",
    "        \"EPOCH\": 200,\n",
    "        \"BATCH_SIZE\": 8,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    SEED = 42\n",
    "    \n",
    "    DENOISER_LR = 3e-4\n",
    "    CLASSIFIER_LR = 3e-4\n",
    "    \n",
    "    EPOCH = 200\n",
    "    BATCH_SIZE = 8\n",
    "    TARGET_SIZE = (128, 128)  \n",
    "\n",
    "    SR = 25600\n",
    "    N_MEL = 128\n",
    "    DURATION = 0.1\n",
    "    #NUM_AUGMENTATIONS = 10\n",
    "    N_FFT = 2048\n",
    "    HOP_LENGTH = 32\n",
    "\n",
    "    NOISE_DIR = 'background_noises'\n",
    "    DATA_DIR = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG.SEED) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_reverb(signal, sr, reverb_amount=0.75, start_time=0.05):\n",
    "    impulse_response = np.concatenate([np.zeros(int(sr * 0.01)), np.ones(int(sr * 0.1))])\n",
    "    reverb_signal = convolve(signal, impulse_response, mode='full')\n",
    "    reverb_signal = reverb_signal[:len(signal)]\n",
    "    start_sample = int(sr * start_time)\n",
    "    padded_reverb_signal = np.concatenate([np.zeros(start_sample), reverb_signal])\n",
    "    padded_reverb_signal = padded_reverb_signal[:len(signal)]\n",
    "    reverb_signal = signal + reverb_amount * padded_reverb_signal\n",
    "    return reverb_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_target_size(mel_spec, target_size):\n",
    "    c, h, w = mel_spec.shape\n",
    "    target_h, target_w = target_size\n",
    "\n",
    "\n",
    "    pad_h = max(0, target_h - h)\n",
    "    pad_w = max(0, target_w - w)\n",
    "\n",
    "    assert pad_h == 0, \"height padding will be occured (N_MEL = 128)\"\n",
    "\n",
    "    pad_top = pad_h // 2\n",
    "    pad_bottom = pad_h - pad_top\n",
    "    pad_left = pad_w // 2\n",
    "    pad_right = pad_w - pad_left\n",
    "\n",
    "    mel_spec_padded = F.pad(mel_spec, (pad_left, pad_right, pad_top, pad_bottom), mode='constant', value=0)\n",
    "    \n",
    "    return mel_spec_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaling(tensor, min_value=0.0, max_value=1.0):\n",
    "    tensor_min = tensor.min()\n",
    "    tensor_max = tensor.max()\n",
    "    \n",
    "    scaled_tensor = (tensor - tensor_min) / (tensor_max - tensor_min)\n",
    "    scaled_tensor = scaled_tensor * (max_value - min_value) + min_value\n",
    "    \n",
    "    return scaled_tensor, tensor_min, tensor_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data_dir = './train1'\n",
    "        self.noise_dir = './background_noises'\n",
    "        self.transform = Compose([AddBackgroundNoise(sounds_path=self.noise_dir, min_snr_in_db=10, max_snr_in_db=10, p=1)])\n",
    "        self.file_paths, self.labels = self._load_file_paths_and_labels()\n",
    "\n",
    "\n",
    "    def _load_file_paths_and_labels(self):\n",
    "        file_paths = []\n",
    "        labels = []\n",
    "\n",
    "        label_map = {'Caution': 0, 'Fault': 1, 'Normal': 2}\n",
    "        for label, idx in label_map.items():\n",
    "            label_dir = os.path.join(self.data_dir, label)\n",
    "            if not os.path.exists(label_dir):\n",
    "                continue\n",
    "            wav_files = [os.path.join(label_dir, f) for f in os.listdir(label_dir) if f.endswith('.wav')]\n",
    "            if wav_files:\n",
    "                file_paths.extend(wav_files)\n",
    "                labels.extend([idx] * len(wav_files))\n",
    "\n",
    "        return file_paths, labels\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #print(\"G\")\n",
    "        file_idx = idx\n",
    "        file_path = self.file_paths[file_idx]\n",
    "        label = self.labels[file_idx]\n",
    "        \n",
    "        y, sr = librosa.load(file_path, sr=CONFIG.SR)\n",
    "\n",
    "        mel = librosa.feature.melspectrogram(\n",
    "            y=y, \n",
    "            sr=CONFIG.SR, \n",
    "            n_mels=CONFIG.N_MEL,\n",
    "            n_fft=CONFIG.N_FFT,\n",
    "            hop_length=CONFIG.HOP_LENGTH\n",
    "        )\n",
    "\n",
    "        #print(\"F\")\n",
    "\n",
    "        log_mel = librosa.power_to_db(mel, ref=np.max)\n",
    "        log_mel = torch.tensor(log_mel).unsqueeze(0).float()\n",
    "        \n",
    "        #print(\"0\")\n",
    "        # Pad the Mel spectrogram to the target size\n",
    "        log_mel_pad = pad_to_target_size(log_mel, target_size=CONFIG.TARGET_SIZE)\n",
    "\n",
    "        #print(\"1\")\n",
    "\n",
    "        y_reverb = add_reverb(y, sr)\n",
    "        y_reverb = np.array(y_reverb, dtype=np.float32)\n",
    "        #print(\"2\")\n",
    "\n",
    "        y_reverb_noise = self.transform(samples=y_reverb, sample_rate=CONFIG.SR)\n",
    "        #print(\"3\")\n",
    "\n",
    "        mel_reverb_noise = librosa.feature.melspectrogram(\n",
    "            y=y_reverb_noise, \n",
    "            sr=CONFIG.SR, \n",
    "            n_mels=CONFIG.N_MEL,\n",
    "            n_fft=CONFIG.N_FFT,\n",
    "            hop_length=CONFIG.HOP_LENGTH\n",
    "        )\n",
    "        #print(\"4\")\n",
    "\n",
    "\n",
    "        log_mel_reverb_noise = librosa.power_to_db(mel_reverb_noise, ref=np.max)\n",
    "        log_mel_reverb_noise = torch.tensor(log_mel_reverb_noise).unsqueeze(0).float()\n",
    "        #print(\"5\")\n",
    "        # Pad the Mel spectrogram to the target size\n",
    "        log_mel_reverb_noise_pad = pad_to_target_size(log_mel_reverb_noise, target_size=CONFIG.TARGET_SIZE)\n",
    "        #print(\"E\")\n",
    "\n",
    "\n",
    "        scaled_log_mel_pad, tensor_min, tensor_max = min_max_scaling(log_mel_pad)\n",
    "        scaled_log_mel_reverb_noise_pad, _, _ = min_max_scaling(log_mel_reverb_noise_pad)\n",
    "\n",
    "        #print(\"B\")\n",
    "\n",
    "        return scaled_log_mel_pad, scaled_log_mel_reverb_noise_pad, label, tensor_min, tensor_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data_dir = './val'\n",
    "        self.base_dir = './val1'\n",
    "        self.file_paths, self.labels = self._load_file_paths_and_labels()\n",
    "\n",
    "\n",
    "    def _load_file_paths_and_labels(self):\n",
    "        file_paths = []\n",
    "        labels = []\n",
    "\n",
    "        label_map = {'Caution': 0, 'Fault': 1, 'Normal': 2}\n",
    "        for label, idx in label_map.items():\n",
    "            label_dir = os.path.join(self.data_dir, label)\n",
    "            if not os.path.exists(label_dir):\n",
    "                continue\n",
    "            wav_files = [os.path.join(label_dir, f) for f in os.listdir(label_dir) if f.endswith('.wav')]\n",
    "            if wav_files:\n",
    "                file_paths.extend(wav_files)\n",
    "                labels.extend([idx] * len(wav_files))\n",
    "\n",
    "        return file_paths, labels\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx = idx\n",
    "        file_path = self.file_paths[file_idx]\n",
    "        label = self.labels[file_idx]\n",
    "\n",
    "        file_path_split = file_path.split('/')\n",
    "\n",
    "        base_path = self.base_dir + '/' + file_path_split[2] + '/' + file_path_split[3].split('_')[0] + '.wav'\n",
    "        \n",
    "        base_y, sr = librosa.load(base_path, sr=CONFIG.SR)\n",
    "\n",
    "        base_mel = librosa.feature.melspectrogram(\n",
    "            y=base_y,\n",
    "            sr=CONFIG.SR, \n",
    "            n_mels=CONFIG.N_MEL,\n",
    "            n_fft=CONFIG.N_FFT,\n",
    "            hop_length=CONFIG.HOP_LENGTH\n",
    "        )\n",
    "\n",
    "        base_log_mel = librosa.power_to_db(base_mel, ref=np.max)\n",
    "        base_log_mel = torch.tensor(base_log_mel).unsqueeze(0).float()\n",
    "        \n",
    "        # Pad the Mel spectrogram to the target size\n",
    "        base_log_mel_pad = pad_to_target_size(base_log_mel, target_size=CONFIG.TARGET_SIZE)\n",
    "\n",
    "        y_reverb_noise, sr = librosa.load(file_path, sr=CONFIG.SR)\n",
    "\n",
    "        \n",
    "        mel_reverb_noise = librosa.feature.melspectrogram(\n",
    "            y=y_reverb_noise, \n",
    "            sr=CONFIG.SR, \n",
    "            n_mels=CONFIG.N_MEL,\n",
    "            n_fft=CONFIG.N_FFT,\n",
    "            hop_length=CONFIG.HOP_LENGTH\n",
    "        )\n",
    "\n",
    "        log_mel_reverb_noise = librosa.power_to_db(mel_reverb_noise, ref=np.max)\n",
    "        log_mel_reverb_noise = torch.tensor(log_mel_reverb_noise).unsqueeze(0).float()\n",
    "        \n",
    "        # Pad the Mel spectrogram to the target size\n",
    "        log_mel_reverb_noise_pad = pad_to_target_size(log_mel_reverb_noise, target_size=CONFIG.TARGET_SIZE)\n",
    "\n",
    "        scaled_log_mel_pad, tensor_min, tensor_max = min_max_scaling(base_log_mel_pad)\n",
    "        scaled_log_mel_reverb_noise_pad, _, _ = min_max_scaling(log_mel_reverb_noise_pad)\n",
    "\n",
    "\n",
    "        return scaled_log_mel_pad, scaled_log_mel_reverb_noise_pad, label, tensor_min, tensor_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data_dir = './test_50'\n",
    "        self.base_dir = './test1'\n",
    "\n",
    "        self.file_paths, self.labels = self._load_file_paths_and_labels()\n",
    "\n",
    "\n",
    "    def _load_file_paths_and_labels(self):\n",
    "        file_paths = []\n",
    "        labels = []\n",
    "\n",
    "        label_map = {'Caution': 0, 'Fault': 1, 'Normal': 2}\n",
    "        for label, idx in label_map.items():\n",
    "            label_dir = os.path.join(self.data_dir, label)\n",
    "            if not os.path.exists(label_dir):\n",
    "                continue\n",
    "            wav_files = [os.path.join(label_dir, f) for f in os.listdir(label_dir) if f.endswith('.wav')]\n",
    "            if wav_files:\n",
    "                file_paths.extend(wav_files)\n",
    "                labels.extend([idx] * len(wav_files))\n",
    "\n",
    "        return file_paths, labels\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx = idx\n",
    "        file_path = self.file_paths[file_idx]\n",
    "        label = self.labels[file_idx]\n",
    "\n",
    "        file_path_split = file_path.split('/')\n",
    "\n",
    "        base_path = self.base_dir + '/' + file_path_split[2] + '/' + file_path_split[3].split('_')[0] + '.wav'\n",
    "\n",
    "        denoise_output_path = \"./dunet_denoised/\" + file_path_split[2] + '/' + file_path_split[3]\n",
    "\n",
    "        base_y, sr = librosa.load(base_path, sr=CONFIG.SR)\n",
    "\n",
    "        base_mel = librosa.feature.melspectrogram(\n",
    "            y=base_y,\n",
    "            sr=CONFIG.SR, \n",
    "            n_mels=CONFIG.N_MEL,\n",
    "            n_fft=CONFIG.N_FFT,\n",
    "            hop_length=CONFIG.HOP_LENGTH\n",
    "        )\n",
    "        \n",
    "        \n",
    "        base_log_mel = librosa.power_to_db(base_mel, ref=np.max)\n",
    "        base_log_mel = torch.tensor(base_log_mel).unsqueeze(0).float()\n",
    "        \n",
    "        # Pad the Mel spectrogram to the target size\n",
    "        base_log_mel_pad = pad_to_target_size(base_log_mel, target_size=CONFIG.TARGET_SIZE)\n",
    "\n",
    "        y_reverb_noise, sr = librosa.load(file_path, sr=CONFIG.SR)\n",
    "        \n",
    "\n",
    "        mel_reverb_noise = librosa.feature.melspectrogram(\n",
    "            y=y_reverb_noise, \n",
    "            sr=CONFIG.SR, \n",
    "            n_mels=CONFIG.N_MEL,\n",
    "            n_fft=CONFIG.N_FFT,\n",
    "            hop_length=CONFIG.HOP_LENGTH\n",
    "        )\n",
    "\n",
    "        log_mel_reverb_noise = librosa.power_to_db(mel_reverb_noise, ref=np.max)\n",
    "        log_mel_reverb_noise = torch.tensor(log_mel_reverb_noise).unsqueeze(0).float()\n",
    "        \n",
    "        # Pad the Mel spectrogram to the target size\n",
    "        log_mel_reverb_noise_pad = pad_to_target_size(log_mel_reverb_noise, target_size=CONFIG.TARGET_SIZE)\n",
    "\n",
    "        # print(base_log_mel_pad)\n",
    "\n",
    "        scaled_log_mel_pad, _, _ = min_max_scaling(base_log_mel_pad)\n",
    "        scaled_log_mel_reverb_noise_pad, tensor_min, tensor_max = min_max_scaling(log_mel_reverb_noise_pad)\n",
    "\n",
    "\n",
    "        return scaled_log_mel_pad, scaled_log_mel_reverb_noise_pad, label, tensor_min, tensor_max, denoise_output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainDataset()\n",
    "val_dataset = ValidDataset()\n",
    "test_dataset = TestDataset()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DUNET(nn.Module):\n",
    "    def __init__(self,in_channels=1, out_channels=1, base_channels=64):\n",
    "        super(DUNET, self).__init__()\n",
    "        base_width = base_channels\n",
    "        self.encoder = UNetEncoder(in_channels, base_width)\n",
    "        self.bottleneck = LinearBottleneck()\n",
    "        self.decoder = UNetDecoder(base_width, out_channels=out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # eb1,eb2,eb3,eb4,eb5,eb6,eb7,eb8,eb9,encoder_output = self.encoder(x)\n",
    "        eb1,eb2,eb3,eb4,eb5,eb6,eb7,encoder_output = self.encoder(x)\n",
    "\n",
    "        bottleneck_output = self.bottleneck(encoder_output)\n",
    "        \n",
    "        # decoder_output = self.decoder(bottleneck_output,eb1,eb2,eb3,eb4,eb5,eb6,eb7,eb8,eb9)\n",
    "        decoder_output = self.decoder(bottleneck_output,eb1,eb2,eb3,eb4,eb5,eb6,eb7)\n",
    "\n",
    "        return decoder_output\n",
    "        \n",
    "\n",
    "class UNetEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, base_width):\n",
    "        super(UNetEncoder, self).__init__()\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, base_width, kernel_size=1),\n",
    "            nn.Conv2d(base_width, base_width, kernel_size=3, dilation=1, padding=1),\n",
    "            nn.PReLU(),\n",
    "            nn.BatchNorm2d(base_width),\n",
    "            nn.Conv2d(base_width, base_width, kernel_size=1))\n",
    "        self.mp1 = nn.Sequential(nn.MaxPool2d(2))\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(base_width, base_width*2, kernel_size=1),\n",
    "            nn.Conv2d(base_width*2, base_width*2, kernel_size=3, dilation=2, padding=2),\n",
    "            nn.PReLU(),\n",
    "            nn.BatchNorm2d(base_width*2),\n",
    "            nn.Conv2d(base_width*2, base_width*2, kernel_size=1))\n",
    "        self.mp2 = nn.Sequential(nn.MaxPool2d(2))\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(base_width*2, base_width*4, kernel_size=1),\n",
    "            nn.Conv2d(base_width*4, base_width*4, kernel_size=3, dilation=3, padding=3),\n",
    "            nn.PReLU(),\n",
    "            nn.BatchNorm2d(base_width*4),\n",
    "            nn.Conv2d(base_width*4, base_width*4, kernel_size=1))\n",
    "        self.mp3 = nn.Sequential(nn.MaxPool2d(2))\n",
    "\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(base_width*4, base_width*8, kernel_size=1),\n",
    "            nn.Conv2d(base_width*8, base_width*8, kernel_size=3, dilation=4, padding=4),\n",
    "            nn.PReLU(),\n",
    "            nn.BatchNorm2d(base_width*8),\n",
    "            nn.Conv2d(base_width*8, base_width*8, kernel_size=1))\n",
    "        self.mp4 = nn.Sequential(nn.MaxPool2d(2))\n",
    "\n",
    "        self.block5 = nn.Sequential(\n",
    "            nn.Conv2d(base_width*8, base_width*16, kernel_size=1),\n",
    "            nn.Conv2d(base_width*16, base_width*16, kernel_size=3, dilation=5, padding=5),\n",
    "            nn.PReLU(),\n",
    "            nn.BatchNorm2d(base_width*16),\n",
    "            nn.Conv2d(base_width*16, base_width*16, kernel_size=1))\n",
    "        self.mp5 = nn.Sequential(nn.MaxPool2d(2))\n",
    "\n",
    "        self.block6 = nn.Sequential(\n",
    "            nn.Conv2d(base_width*16, base_width*16, kernel_size=1),\n",
    "            nn.Conv2d(base_width*16, base_width*16, kernel_size=3, dilation=6, padding=6),\n",
    "            nn.PReLU(),\n",
    "            nn.BatchNorm2d(base_width*16),\n",
    "            nn.Conv2d(base_width*16, base_width*16, kernel_size=1))\n",
    "        self.mp6 = nn.Sequential(nn.MaxPool2d(2))\n",
    "\n",
    "        self.block7 = nn.Sequential(\n",
    "            nn.Conv2d(base_width*16, base_width*16, kernel_size=1),\n",
    "            nn.Conv2d(base_width*16, base_width*16, kernel_size=3, dilation=7, padding=7),\n",
    "            nn.PReLU(),\n",
    "            nn.BatchNorm2d(base_width*16),\n",
    "            nn.Conv2d(base_width*16, base_width*16, kernel_size=1))\n",
    "        self.mp7 = nn.Sequential(nn.MaxPool2d(2))\n",
    "        \n",
    "        # self.block8 = nn.Sequential(\n",
    "        #     nn.Conv2d(base_width*16, base_width*16, kernel_size=1),\n",
    "        #     nn.Conv2d(base_width*16, base_width*16, kernel_size=3, dilation=8, padding=8),\n",
    "        #     nn.PReLU(),\n",
    "        #     nn.BatchNorm2d(base_width*16),\n",
    "        #     nn.Conv2d(base_width*16, base_width*16, kernel_size=1))\n",
    "        # self.mp8 = nn.Sequential(nn.MaxPool2d(2))\n",
    "\n",
    "        # self.block9 = nn.Sequential(\n",
    "        #     nn.Conv2d(base_width*16, base_width*16, kernel_size=1),\n",
    "        #     nn.Conv2d(base_width*16, base_width*16, kernel_size=3, dilation=9, padding=9),\n",
    "        #     nn.PReLU(),\n",
    "        #     nn.BatchNorm2d(base_width*16),\n",
    "        #     nn.Conv2d(base_width*16, base_width*16, kernel_size=1))\n",
    "        # self.mp9 = nn.Sequential(nn.MaxPool2d(2))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        eb1 = self.block1(x)\n",
    "        mp1 = self.mp1(eb1)\n",
    "        eb2 = self.block2(mp1)\n",
    "        mp2 = self.mp2(eb2)\n",
    "        eb3 = self.block3(mp2)\n",
    "        mp3 = self.mp3(eb3)\n",
    "        eb4 = self.block4(mp3)\n",
    "        mp4 = self.mp4(eb4)\n",
    "        eb5 = self.block5(mp4)\n",
    "        mp5 = self.mp5(eb5)\n",
    "        eb6 = self.block6(mp5)\n",
    "        mp6 = self.mp6(eb6)\n",
    "        eb7 = self.block7(mp6)\n",
    "        encoder_output = self.mp7(eb7)\n",
    "\n",
    "        # mp7 = self.mp7(eb7)\n",
    "        # eb8 = self.block8(mp7)\n",
    "        # mp8 = self.mp8(eb8)\n",
    "        # eb9 = self.block9(mp8)\n",
    "        # encoder_output = self.mp9(eb9)\n",
    "        \n",
    "        # return eb1,eb2,eb3,eb4,eb5,eb6,eb7,eb8,eb9,encoder_output\n",
    "        return eb1,eb2,eb3,eb4,eb5,eb6,eb7,encoder_output\n",
    "\n",
    "\n",
    "\n",
    "class LinearBottleneck(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearBottleneck, self).__init__()\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.PReLU(),\n",
    "            nn.Unflatten(1, (1024, 1, 1)))\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNetDecoder(nn.Module):\n",
    "    def __init__(self, base_width, out_channels=1):\n",
    "        super(UNetDecoder, self).__init__()\n",
    "\n",
    "        self.tp9 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_width*16, base_width*16, stride=2, kernel_size=2))\n",
    "        self.block9 = nn.Sequential(\n",
    "            nn.Conv2d(base_width*(16+16), base_width*16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(base_width*16),\n",
    "            nn.Conv2d(base_width*16, base_width*16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(base_width*16))\n",
    "\n",
    "        self.tp8 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_width*16, base_width*16, stride=2, kernel_size=2))\n",
    "        self.block8 = nn.Sequential(\n",
    "            nn.Conv2d(base_width*(16+16), base_width*16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(base_width*16),\n",
    "            nn.Conv2d(base_width*16, base_width*16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(base_width*16))\n",
    "        \n",
    "        self.tp7 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_width*16, base_width*16, stride=2, kernel_size=2))\n",
    "        self.block7 = nn.Sequential(\n",
    "            nn.Conv2d(base_width*(16+16), base_width*16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(base_width*16),\n",
    "            nn.Conv2d(base_width*16, base_width*16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(base_width*16))\n",
    "        \n",
    "        self.tp6 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_width*16, base_width*16, stride=2, kernel_size=2))\n",
    "        self.block6 = nn.Sequential(\n",
    "            nn.Conv2d(base_width*(16+16), base_width*16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(base_width*16),\n",
    "            nn.Conv2d(base_width*16, base_width*16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(base_width*16))\n",
    "        \n",
    "        self.tp5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_width*16, base_width*16, stride=2, kernel_size=2))\n",
    "        self.block5 = nn.Sequential(\n",
    "            nn.Conv2d(base_width*(16+16), base_width*16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(base_width*16),\n",
    "            nn.Conv2d(base_width*16, base_width*16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(base_width*16))\n",
    "        \n",
    "        self.tp4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_width*16, base_width*8, stride=2, kernel_size=2))\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(base_width*(8+8), base_width*8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(base_width*8),\n",
    "            nn.Conv2d(base_width*8, base_width*8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(base_width*8))\n",
    "        \n",
    "        self.tp3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_width*8, base_width*4, stride=2, kernel_size=2))\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(base_width*(4+4), base_width*4, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(base_width*4),\n",
    "            nn.Conv2d(base_width*4, base_width*4, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(base_width*4))\n",
    "        \n",
    "        self.tp2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_width*4, base_width*2, stride=2, kernel_size=2))\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(base_width*(2+2), base_width*2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(base_width*2),\n",
    "            nn.Conv2d(base_width*2, base_width*2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(base_width*2))\n",
    "        \n",
    "        self.tp1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_width*2, base_width, stride=2, kernel_size=2))\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(base_width*(1+1), base_width, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(base_width),\n",
    "            nn.Conv2d(base_width, base_width, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(base_width))\n",
    "        \n",
    "        self.block10 = nn.Sequential(\n",
    "            nn.Conv2d(base_width, out_channels, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "\n",
    "    # def forward(self,bottleneck_output,eb1,eb2,eb3,eb4,eb5,eb6,eb7,eb8,eb9):\n",
    "    def forward(self,bottleneck_output,eb1,eb2,eb3,eb4,eb5,eb6,eb7):\n",
    "        # tp9 = self.tp9(bottleneck_output)\n",
    "        # cat9 = torch.cat((tp9, eb9), dim=1)\n",
    "        # db9 = self.block9(cat9)\n",
    "\n",
    "        # tp8 = self.tp8(db9)\n",
    "        # cat8 = torch.cat((tp8, eb8), dim=1)\n",
    "        # db8 = self.block8(cat8)\n",
    "\n",
    "        # tp7 = self.tp7(db8)\n",
    "        tp7 = self.tp7(bottleneck_output)\n",
    "        cat7 = torch.cat((tp7, eb7), dim=1)\n",
    "        db7 = self.block7(cat7)\n",
    "\n",
    "        tp6 = self.tp6(db7)\n",
    "        cat6 = torch.cat((tp6, eb6), dim=1)\n",
    "        db6 = self.block6(cat6)\n",
    "\n",
    "        tp5 = self.tp5(db6)\n",
    "        cat5 = torch.cat((tp5, eb5), dim=1)\n",
    "        db5 = self.block5(cat5)\n",
    "\n",
    "        tp4 = self.tp4(db5)\n",
    "        cat4 = torch.cat((tp4, eb4), dim=1)\n",
    "        db4 = self.block4(cat4)\n",
    "\n",
    "        tp3 = self.tp3(db4)\n",
    "        cat3 = torch.cat((tp3, eb3), dim=1)\n",
    "        db3 = self.block3(cat3)\n",
    "\n",
    "        tp2 = self.tp2(db3)\n",
    "        cat2 = torch.cat((tp2, eb2), dim=1)\n",
    "        db2 = self.block2(cat2)\n",
    "\n",
    "        tp1 = self.tp1(db2)\n",
    "        cat9 = torch.cat((tp1, eb1), dim=1)\n",
    "        db1 = self.block1(cat9)\n",
    "\n",
    "        decoder_output = self.block10(db1)\n",
    "\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50 = models.resnet50(pretrained=True)\n",
    "num_ftrs = resnet50.fc.in_features\n",
    "resnet50.fc = nn.Linear(num_ftrs, 3)\n",
    "first_conv_layer = resnet50.conv1\n",
    "original_weights = first_conv_layer.weight.data\n",
    "\n",
    "new_first_conv = nn.Conv2d(1, first_conv_layer.out_channels,\n",
    "                           kernel_size=first_conv_layer.kernel_size,\n",
    "                           stride=first_conv_layer.stride,\n",
    "                           padding=first_conv_layer.padding,\n",
    "                           bias=first_conv_layer.bias)\n",
    "\n",
    "with torch.no_grad():\n",
    "    new_first_conv.weight = nn.Parameter(torch.mean(original_weights, dim=1, keepdim=True))\n",
    "\n",
    "resnet50.conv1 = new_first_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomClassifier(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super(CustomClassifier, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = torch.log_softmax(x, dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0.0, verbose=True):\n",
    "        \"\"\"\n",
    "        patience (int): loss or score가 개선된 후 기다리는 기간. default: 3\n",
    "        delta  (float): 개선시 인정되는 최소 변화 수치. default: 0.0\n",
    "        mode     (str): 개선시 최소/최대값 기준 선정('min' or 'max'). default: 'min'.\n",
    "        verbose (bool): 메시지 출력. default: True\n",
    "        \"\"\"\n",
    "        self.early_stop = False\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        \n",
    "        self.best_mse = np.Inf\n",
    "        self.best_f1 = 0\n",
    "        self.delta = delta\n",
    "        \n",
    "\n",
    "    def __call__(self, mse, f1, denoiser_model, classifier_model):\n",
    "\n",
    "        if f1 > (self.best_f1 - self.delta): \n",
    "            self.counter = 0\n",
    "            self.best_f1 = f1\n",
    "            self.best_mse = mse\n",
    "            if self.verbose:\n",
    "                print(f'(Update) Best MSE: {self.best_mse:.5f} Best F1: {self.best_f1:.5f}')\n",
    "            self.save_checkpoint(denoiser_model, classifier_model)\n",
    "\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'(Patience) {self.counter}/{self.patience} Current MSE: {mse:.5f} Current F1: {f1:.5f} Best MSE: {self.best_mse:.5f} Best F1: {self.best_f1:.5f}')\n",
    "\n",
    "\n",
    "        if self.counter >= self.patience:\n",
    "            if self.verbose:\n",
    "                print(f'[EarlyStopping] (Patience) {self.counter}/{self.patience} Best MSE: {self.best_mse:.5f} Best F1: {self.best_f1:.5f}')\n",
    "            self.early_stop = True\n",
    "                \n",
    "        else:\n",
    "            self.early_stop = False\n",
    "\n",
    "    def save_checkpoint(self, denoiser_model, classifier_model):\n",
    "        if self.verbose:\n",
    "            print(f'Saving model ...')\n",
    "        torch.save(denoiser_model.state_dict(), 'dunet_denoiser_75_es10_N.pt')\n",
    "        torch.save(classifier_model.state_dict(), 'dunet_classifier_75_es10_N.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(denoiser_model, classifier_model, denoiser_optimizer, classifier_optimizer, denoiser_scheduler, classifier_scheduler, train_loader, val_loader, device):\n",
    "    denoiser_model.to(device)\n",
    "    classifier_model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(CONFIG.EPOCH):\n",
    "        denoiser_model.train()\n",
    "        classifier_model.train()\n",
    "        \n",
    "        train_reconstruction_loss, train_regularization_loss, train_classifier_loss = [], [], []\n",
    "\n",
    "        for log_mel_pad, log_mel_reverb_noise_pad, label, _, _ in tqdm(iter(train_loader)):\n",
    "            log_mel_pad = log_mel_pad.float().to(device)\n",
    "            log_mel_reverb_noise_pad = log_mel_reverb_noise_pad.float().to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            denoiser_optimizer.zero_grad()\n",
    "            denoiser_output = denoiser_model(log_mel_reverb_noise_pad)\n",
    "            reconstruction_loss = F.mse_loss(denoiser_output, log_mel_pad)\n",
    "            denoiser_loss = reconstruction_loss\n",
    "            denoiser_loss.backward()\n",
    "            denoiser_optimizer.step()\n",
    "            train_reconstruction_loss.append(reconstruction_loss.item())\n",
    "\n",
    "            wandb.log({\n",
    "                \"Clear\": wandb.Image(log_mel_pad[0].cpu().detach().numpy(), caption=\"Clear\"),\n",
    "                \"Denoiser Output\": wandb.Image(denoiser_output[0].cpu().detach().numpy(), caption=\"Denoised Output\"),\n",
    "                \"Noise\": wandb.Image(log_mel_reverb_noise_pad[0].cpu().detach().numpy(), caption=\"Noise\")\n",
    "            })\n",
    "\n",
    "            classifier_optimizer.zero_grad()\n",
    "            classifier_output = classifier_model(denoiser_output.detach()) # 역전파 방지\n",
    "            classifier_loss = F.cross_entropy(classifier_output, label)\n",
    "            classifier_loss.backward()\n",
    "            classifier_optimizer.step()\n",
    "            train_classifier_loss.append(classifier_loss.item())\n",
    "\n",
    "        denoiser_scheduler.step()\n",
    "        classifier_scheduler.step()\n",
    "\n",
    "        train_reconstruction_loss = np.mean(train_reconstruction_loss)\n",
    "        train_classifier_loss = np.mean(train_classifier_loss)\n",
    "\n",
    "        val_reconstruction_loss, val_classifier_loss, f1 = validation(denoiser_model, classifier_model, val_loader, device)\n",
    "\n",
    "\n",
    "        wandb.log({\n",
    "            \"Train Reconstruction Loss\": train_reconstruction_loss,\n",
    "            \"Train Classification Loss\": train_classifier_loss,\n",
    "            \"Valid Reconstruction Loss\": val_reconstruction_loss,\n",
    "            \"Valid Classification Loss\": val_classifier_loss,\n",
    "            \"F1 Score\": f1,\n",
    "            \"Epoch\": epoch\n",
    "        })\n",
    "\n",
    "        es(val_reconstruction_loss, f1, denoiser_model, classifier_model)\n",
    "\n",
    "        if es.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def validation(denoiser_model, classifier_model, val_loader, device):\n",
    "    denoiser_model.eval()\n",
    "    classifier_model.eval()\n",
    "        \n",
    "    val_reconstruction_loss, val_regularization_loss, val_classifier_loss = [], [], []\n",
    "\n",
    "\n",
    "    preds, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for log_mel_pad, log_mel_reverb_noise_pad, label, _, _ in tqdm(iter(val_loader)):\n",
    "            log_mel_pad = log_mel_pad.float().to(device)\n",
    "            log_mel_reverb_noise_pad = log_mel_reverb_noise_pad.float().to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            denoiser_output = denoiser_model(log_mel_reverb_noise_pad)\n",
    "            reconstruction_loss = F.mse_loss(denoiser_output, log_mel_pad)\n",
    "            denoiser_loss = reconstruction_loss \n",
    "            val_reconstruction_loss.append(reconstruction_loss.item())\n",
    "\n",
    "            classifier_output = classifier_model(denoiser_output.detach()) # 역전파 방지\n",
    "            classifier_loss = F.cross_entropy(classifier_output, label)\n",
    "            val_classifier_loss.append(classifier_loss.item())\n",
    "\n",
    "            pred = classifier_output.argmax(dim=1)\n",
    "\n",
    "            preds.append(pred.cpu().numpy())\n",
    "            labels.append(label.cpu().numpy())\n",
    "\n",
    "        val_reconstruction_loss = np.mean(val_reconstruction_loss)\n",
    "        val_classifier_loss = np.mean(val_classifier_loss)\n",
    "\n",
    "        preds = np.concatenate(preds, axis=0)\n",
    "        labels = np.concatenate(labels, axis=0)\n",
    "\n",
    "        f1 = f1_score(labels, preds, average='weighted')\n",
    "\n",
    "    return  val_reconstruction_loss, val_classifier_loss, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoiser_model = DUNET()\n",
    "denoiser_optimizer = torch.optim.AdamW(params=denoiser_model.parameters(), lr=CONFIG.DENOISER_LR)\n",
    "denoiser_scheduler = torch.optim.lr_scheduler.LambdaLR(denoiser_optimizer, lr_lambda = lambda epoch: 1.0 ** CONFIG.EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifer_model = CustomClassifier(backbone=resnet50)\n",
    "classifier_optimizer = torch.optim.AdamW(params=classifer_model.parameters(), lr=CONFIG.CLASSIFIER_LR)\n",
    "classifier_scheduler = torch.optim.lr_scheduler.LambdaLR(classifier_optimizer, lr_lambda = lambda epoch: 1.0 ** CONFIG.EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(denoiser_model, classifer_model, denoiser_optimizer, classifier_optimizer, denoiser_scheduler, classifier_scheduler, train_loader, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_denoiser_model = DUNET()\n",
    "ckpt_denoiser_model.load_state_dict(torch.load('./dunet_denoiser_50_es10.pt'))\n",
    "ckpt_denoiser_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_classifier_model = CustomClassifier(backbone=resnet50)\n",
    "ckpt_classifier_model.load_state_dict(torch.load('./dunet_classifier_50_es10.pt'))\n",
    "ckpt_classifier_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_min_max_scaling(scaled_tensor, tensor_min, tensor_max, min_value=0.0, max_value=1.0):\n",
    "    tensor_min = tensor_min.numpy()\n",
    "    tensor_max = tensor_max.numpy()\n",
    "    original_tensor = (scaled_tensor - min_value) / (max_value - min_value)\n",
    "    original_tensor = original_tensor * (tensor_max - tensor_min) + tensor_min\n",
    "    \n",
    "    return original_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(best_denoiser_model, best_classifier_model, test_loader, device):\n",
    "\n",
    "    best_denoiser_model.to(device)\n",
    "    best_classifier_model.to(device)\n",
    "    best_denoiser_model.eval()\n",
    "    best_classifier_model.eval()\n",
    "\n",
    "    preds, labels = [], []\n",
    "\n",
    "    mses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for log_mel_pad, log_mel_reverb_noise_pad, label, tensor_min, tensor_max, denoise_output_path in tqdm(iter(test_loader)):\n",
    "\n",
    "            log_mel_reverb_noise_pad = log_mel_reverb_noise_pad.float().to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            denoiser_output = best_denoiser_model(log_mel_reverb_noise_pad)\n",
    "            for i, log_mel_spec in enumerate(denoiser_output.detach().cpu().numpy()):\n",
    "                log_mel_spec = np.squeeze(log_mel_spec, axis=0)\n",
    "                lp = np.squeeze(log_mel_pad[i].numpy(), axis=0)\n",
    "                #plt.imsave(bases_path[i]+'.png', lp, cmap=\"gray\")\n",
    "                mse = np.mean((lp - log_mel_spec) ** 2)\n",
    "                mses.append(mse)\n",
    "            #     log_mel_spec = inverse_min_max_scaling(log_mel_spec, tensor_min[i], tensor_max[i])\n",
    "                \n",
    "            #     mel_spec = librosa.db_to_power(log_mel_spec)\n",
    "\n",
    "            #     #mel_spec_audio = librosa.feature.inverse.mel_to_audio(mel_spec, sr=CONFIG.SR, n_fft=CONFIG.N_FFT, hop_length=CONFIG.HOP_LENGTH)\n",
    "            #     S = librosa.feature.inverse.mel_to_stft(mel_spec, sr=CONFIG.SR, n_fft=CONFIG.N_FFT)\n",
    "\n",
    "            #     mel_spec_audio = librosa.griffinlim(S, n_fft=CONFIG.N_FFT, hop_length=CONFIG.HOP_LENGTH)\n",
    "            #     mel_spec_audio = np.squeeze(mel_spec_audio)\n",
    "            #     #print(mel_spec_audio.shape)\n",
    "            #     sf.write(denoise_output_path[i], mel_spec_audio, CONFIG.SR, format='WAV')\n",
    "\n",
    "            classifier_output = best_classifier_model(denoiser_output.detach()) # 역전파 방지\n",
    "\n",
    "\n",
    "            pred = classifier_output.argmax(dim=1)\n",
    "\n",
    "            preds.append(pred.cpu().numpy())\n",
    "            labels.append(label.cpu().numpy())\n",
    "\n",
    "\n",
    "        preds = np.concatenate(preds, axis=0)\n",
    "        labels = np.concatenate(labels, axis=0)\n",
    "\n",
    "        f1 = f1_score(labels, preds, average='weighted')\n",
    "        print(f'Test F1 Score: {f1:.4f}')\n",
    "        print(f'Test MSE: {np.mean(mses):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(ckpt_denoiser_model, ckpt_classifier_model, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
